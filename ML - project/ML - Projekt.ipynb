{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21488de5",
   "metadata": {},
   "source": [
    "# Sprawdzenie i oczyszczenie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3cade6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importuję potrzebne biblioteki. \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Importuję zbiór danych.\n",
    "df1 = pd.read_excel(\"Dataset Basic.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d2fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sprawdzam jak wygląda zbiór danych.\n",
    "#df1.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8888cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z9</th>\n",
       "      <th>Z10</th>\n",
       "      <th>Z11</th>\n",
       "      <th>Z12</th>\n",
       "      <th>Z13</th>\n",
       "      <th>Z14</th>\n",
       "      <th>Z15</th>\n",
       "      <th>Z16</th>\n",
       "      <th>Z17</th>\n",
       "      <th>Z18</th>\n",
       "      <th>Z19</th>\n",
       "      <th>Z20</th>\n",
       "      <th>Z21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>640</td>\n",
       "      <td>639</td>\n",
       "      <td>638</td>\n",
       "      <td>639</td>\n",
       "      <td>382</td>\n",
       "      <td>551</td>\n",
       "      <td>273</td>\n",
       "      <td>58</td>\n",
       "      <td>36</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>375</td>\n",
       "      <td>105</td>\n",
       "      <td>45</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>443</td>\n",
       "      <td>620</td>\n",
       "      <td>265</td>\n",
       "      <td>521</td>\n",
       "      <td>6</td>\n",
       "      <td>72</td>\n",
       "      <td>75</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Z9  Z10  Z11  Z12  Z13  Z14  Z15  Z16  Z17  Z18  Z19  Z20  Z21\n",
       "count   640  639  638  639  382  551  273   58   36   63    7    9  502\n",
       "unique    4    4   31    4  375  105   45   12   17   14    5    4   88\n",
       "top       1    0    1    0    0    1    1    1    1    1    1    1    2\n",
       "freq    443  620  265  521    6   72   75   36   10   38    3    6   43"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sprawdzam podstawowe statystyki dotyczące zmiennych.\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c00b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 644 entries, 0 to 643\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Z9      640 non-null    object\n",
      " 1   Z10     639 non-null    object\n",
      " 2   Z11     638 non-null    object\n",
      " 3   Z12     639 non-null    object\n",
      " 4   Z13     382 non-null    object\n",
      " 5   Z14     551 non-null    object\n",
      " 6   Z15     273 non-null    object\n",
      " 7   Z16     58 non-null     object\n",
      " 8   Z17     36 non-null     object\n",
      " 9   Z18     63 non-null     object\n",
      " 10  Z19     7 non-null      object\n",
      " 11  Z20     9 non-null      object\n",
      " 12  Z21     502 non-null    object\n",
      "dtypes: object(13)\n",
      "memory usage: 65.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68ca062",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop('Z13',axis=1) #Usuwam zmienną Z13, ponieważ nie będzie wykorzystywana w sieci. \n",
    "df1 = df1.drop([0,1]) #Usuwam pierwsze dwa wiersze, które służą jako opis dla zmiennych.\n",
    "\n",
    "df1['Z9'] = df1['Z9'].astype(bool) #Zmieniam typ zmiennych, które przyjmują wartości 0 lub 1 na boolean (wartości logiczne - prawda lub fałsz).\n",
    "df1['Z10'] = df1['Z9'].astype(bool)\n",
    "df1['Z12'] = df1['Z9'].astype(bool)\n",
    "\n",
    "#Po przeglądnięciu zbioru postanowiłem, że najlepszym sposobem na pozbycie się brakujących danych będzie zamienienie ich na 0.\n",
    "#Przyjmuję więc, że brak danych jest równoznaczny z tym, że reakcji/obrazu po prostu nie było.\n",
    "df1['Z11'].fillna(0, inplace=True) \n",
    "df1['Z14'].fillna(0, inplace=True)\n",
    "df1['Z15'].fillna(0, inplace=True)\n",
    "df1['Z16'].fillna(0, inplace=True)\n",
    "df1['Z17'].fillna(0, inplace=True)\n",
    "df1['Z18'].fillna(0, inplace=True)\n",
    "df1['Z19'].fillna(0, inplace=True)\n",
    "df1['Z20'].fillna(0, inplace=True)\n",
    "df1['Z21'].fillna(0, inplace=True)\n",
    "\n",
    "#Resetuję index w związku z usunięciem dwóch pierwszych wierszy.\n",
    "df1 = df1.reset_index() \n",
    "df1 = df1.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5715ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tak wygląda gotowy zbiór danych.\n",
    "#df1.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20165fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z11</th>\n",
       "      <th>Z14</th>\n",
       "      <th>Z15</th>\n",
       "      <th>Z16</th>\n",
       "      <th>Z17</th>\n",
       "      <th>Z18</th>\n",
       "      <th>Z19</th>\n",
       "      <th>Z20</th>\n",
       "      <th>Z21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>642.000000</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>642.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.376947</td>\n",
       "      <td>26.512461</td>\n",
       "      <td>4.766355</td>\n",
       "      <td>0.903427</td>\n",
       "      <td>0.506231</td>\n",
       "      <td>0.409657</td>\n",
       "      <td>0.461059</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>17.526480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.840947</td>\n",
       "      <td>95.635834</td>\n",
       "      <td>24.150818</td>\n",
       "      <td>18.329963</td>\n",
       "      <td>4.438253</td>\n",
       "      <td>4.178778</td>\n",
       "      <td>11.563804</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>41.950235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>78.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>472.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>293.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>547.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Z11          Z14         Z15         Z16         Z17  \\\n",
       "count  642.000000   642.000000  642.000000  642.000000  642.000000   \n",
       "mean     2.376947    26.512461    4.766355    0.903427    0.506231   \n",
       "std      5.840947    95.635834   24.150818   18.329963    4.438253   \n",
       "min      0.000000     0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000     1.000000    0.000000    0.000000    0.000000   \n",
       "50%      1.000000     6.000000    0.000000    0.000000    0.000000   \n",
       "75%      2.000000    18.000000    2.000000    0.000000    0.000000   \n",
       "max     78.000000  1600.000000  472.000000  464.000000   81.000000   \n",
       "\n",
       "              Z18         Z19         Z20         Z21  \n",
       "count  642.000000  642.000000  642.000000  642.000000  \n",
       "mean     0.409657    0.461059    0.009346   17.526480  \n",
       "std      4.178778   11.563804    0.096296   41.950235  \n",
       "min      0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000    1.000000  \n",
       "50%      0.000000    0.000000    0.000000    7.000000  \n",
       "75%      0.000000    0.000000    0.000000   17.000000  \n",
       "max     99.000000  293.000000    1.000000  547.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "039f3f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 642 entries, 0 to 641\n",
      "Data columns (total 12 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   Z9      642 non-null    bool \n",
      " 1   Z10     642 non-null    bool \n",
      " 2   Z11     642 non-null    int64\n",
      " 3   Z12     642 non-null    bool \n",
      " 4   Z14     642 non-null    int64\n",
      " 5   Z15     642 non-null    int64\n",
      " 6   Z16     642 non-null    int64\n",
      " 7   Z17     642 non-null    int64\n",
      " 8   Z18     642 non-null    int64\n",
      " 9   Z19     642 non-null    int64\n",
      " 10  Z20     642 non-null    int64\n",
      " 11  Z21     642 non-null    int64\n",
      "dtypes: bool(3), int64(9)\n",
      "memory usage: 47.1 KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec7d6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dodatkowo sprawdzam w Excelu czy wszystko się zgadza.\n",
    "df1.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf7076f",
   "metadata": {},
   "source": [
    "# Pierwsza Iteracja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9de1d8",
   "metadata": {},
   "source": [
    "### Wczytywanie danych do modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec330633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Konwertuję zmienne na tablicę numpy.\n",
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values #Zmienne na podstawie których przewidujemy (cechy wejściowe). \n",
    "y = df1['Z21'].values #Liczba komentarzy, którą chcemy przewidywać (wektor docelowy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d30c3",
   "metadata": {},
   "source": [
    "### Podział danych na zbiór treningowy i testowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "681c3b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zbiór treningowy - 80%, Zbiór testowy - 20%, 'random_state=42' zapewnia powtarzalność podziału danych.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19af10a",
   "metadata": {},
   "source": [
    "### Skalowanie zmiennych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891585d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standaryzuję zbiór treningowy za pomocą StandardScaler z biblioteki sklearn. \n",
    "#Następnie przeprowadzam takie samo skalowanie na zbiorze testowym, zapewniając spójność między obydwoma zbiorami.\n",
    "#Standardyzacja przekształca dane tak, aby miały średnią równą zero i wariancję równą jeden.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753bb6b5",
   "metadata": {},
   "source": [
    "### Przygotowanie architektury sieci (Perceptron)\n",
    "Definiuje klasę 'Perceptron', która implementuje algorytm perceptronu złożony z pojedynczego sztucznego neuronu. Jego celem jest przewidywanie liczby komentarzy na podstawie podanych zmiennych. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73ecf271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100): \n",
    "        self.num_features = num_features #Liczba cech wejściowych.\n",
    "        self.learning_rate = learning_rate #Współczynnik uczenia (jak szybko model będzie się dostosowywał do danych treningowych).\n",
    "        self.num_epochs = num_epochs #Licza epok (ile razy model przejdzie przez cały zbiór danych treningowych podczas procesu uczenia).\n",
    "        self.weights = np.zeros(num_features + 1) #Wagi początkowe - wektor zer o długości = liczba cech + 1 (dodatkowa jedynka odnosi się do wagi biasu)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights) #Wypisuje wagi początkowe modelu.\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights #Zwraca aktualne wagi modelu.\n",
    "    \n",
    "    def activate(self, x):\n",
    "            return x #Funkcja aktywacji, która zwraca input (x) bez żadnych zmian.\n",
    "\n",
    "    def predict(self, x): #Służy do przewidywania wyniku na podstawie podanego wektora wejściowego x.\n",
    "        x = np.insert(x, 0, 1)  #Dodanie 1 na pozycji zerowej odpowiada za bias.\n",
    "        activation = np.dot(self.weights, x) #Funkcja np.dot z biblioteki NumPy wykonuje iloczyn skalarany między wektorem wag, a wektorem x.\n",
    "        return activation #Wynik zwraca wartość liczbową (aktywację), która stanowi naszą predykcję.\n",
    "    \n",
    "    def train(self, X, y): #Trenowanie perceptronu poprzez aktualizację wag na podstawie błędu predykcji.\n",
    "        for _ in range(self.num_epochs): #Pętla iterująca przez określoną liczbę epok.\n",
    "            for i in range(len(X)): #Pętla iterująca przez wszystkie przykłady treningowe ('i' to indeks bieżącego przykładu).\n",
    "                x = X[i] #Wektor wejściowy (x) dla danego przykładu.\n",
    "                y_true = y[i] #Wartość docelowa dla danego przykładu.\n",
    "                y_pred = self.predict(x) #Obliczenie predykcji dla danego x. \n",
    "                error = y_true - y_pred #Obliczenie błędu predykcji jako różnicy między wartością docelową 'y_true' a predykcją 'y_pred'.\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1) #Aktualizacja wag perceptronu na podstawie błędu predykcji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805f97b",
   "metadata": {},
   "source": [
    "### Inicjalizacja i trening perceptronu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71fcd238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100) #Ustawia liczbę cech na równą liczbie kolumn w zbiorze treningowym + określa współczynnik uczenia i liczbę epok. \n",
    "perceptron.train(X_train_scaled, y_train) #Uczenie modelu za pomocą metody 'train' poprzez przekazanie standaryzowanych danych treningowych oraz danych docelowych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567d416",
   "metadata": {},
   "source": [
    "### Prognozowanie na danych treningowych i testowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "163a13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generuje przewidywane wartości na podstawie wytrenowanego perceptronu dla danych treningowych i testowych.\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_scaled]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_scaled]\n",
    "#print(y_pred_train) #lista przewidywanych wartości dla danych treningowych.\n",
    "#print(y_pred_test) #lista przewidywanych wartości dla danych testowych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5343a8a",
   "metadata": {},
   "source": [
    "### Obliczanie błędu średniokwadratowego (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9283c56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Błąd (treningowe): 3.4578085077500045e+212\n",
      "Błąd (testowe): 7.549259127633164e+210\n"
     ]
    }
   ],
   "source": [
    "#Miara oceny jakości modelu\n",
    "#Oblicza różnicę pomiędzy przewidywanymi wartościami a rzeczywistymi wartościami osobno w zbiorze treningowym i zbiorze testowym.\n",
    "#Różnice zostają podniesione do kwadratu i obliczana jest ich średnia wartość.\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2) \n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (treningowe):', mse_train)\n",
    "print('Błąd (testowe):', mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bcc31a",
   "metadata": {},
   "source": [
    "Wartości błędu są bardzo wysokie, co sprawia, że obecna architektura nie będzie przydatna w tworzeniu predykcji. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f65aad",
   "metadata": {},
   "source": [
    "### Wagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3ff8824",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi końcowe:\n",
      "[-4.21821224e+104  3.63378053e+103  3.63378053e+103 -9.49304435e+103\n",
      "  3.63378053e+103 -4.41918151e+105  7.85555541e+105 -7.22310964e+105\n",
      "  2.03371002e+103 -1.27995074e+105 -7.66462458e+105 -2.34631615e+104]\n"
     ]
    }
   ],
   "source": [
    "trained_weights = perceptron.get_weights() #pobiera wagi modelu, które są rezultatem procesu uczenia (wraz z wagą biasu).\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5915e",
   "metadata": {},
   "source": [
    "Wartości wag reprezenują wpływ (negatywny lub pozytywny) danej zmiennej na liczbę komentarzy oraz siłę tego wpływu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfd86d",
   "metadata": {},
   "source": [
    "### Przykład działania na nowych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1f1a0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba komentarzy: 9.997557902257973e+105\n"
     ]
    }
   ],
   "source": [
    "#Nowe dane\n",
    "new_data = np.array([[False, True, 3, False, 11, 50, 5, 23, 1, 2, 1]])\n",
    "\n",
    "#Standaryzacja nowych danych\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "#Przewidywanie liczby komentarzy\n",
    "prediction = perceptron.predict(new_data_scaled)\n",
    "\n",
    "#Wyświetlenie przewidywanej liczby komentarzy\n",
    "print('Przewidywana liczba komentarzy:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db933b0",
   "metadata": {},
   "source": [
    "Przewidywana liczba komentarzy jest w oczywisty sposób nietrafiona."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04470d0a",
   "metadata": {},
   "source": [
    "# Kolejne Iteracje - wykaz sprawdzonych architektur sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff75c1d4",
   "metadata": {},
   "source": [
    "## Badanie wpływu zmiany typu inputów na wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f82c8f",
   "metadata": {},
   "source": [
    "### Zmiana na Integer (liczby całkowite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62f05e26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Błąd (Treningowe): 3.4578085077500045e+212\n",
      "Błąd (Testowe): 7.549259127633164e+210\n",
      "Wagi końcowe:\n",
      "[-4.21821224e+104  3.63378053e+103  3.63378053e+103 -9.49304435e+103\n",
      "  3.63378053e+103 -4.41918151e+105  7.85555541e+105 -7.22310964e+105\n",
      "  2.03371002e+103 -1.27995074e+105 -7.66462458e+105 -2.34631615e+104]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def activate(self, x):\n",
    "        return x\n",
    "    \n",
    "    #Dodanie funkcji, która konwertuje każdy input x na liczby całkowite (integer).\n",
    "    def preprocess_input(self, x):\n",
    "        processed_input = [int(feature) for feature in x]\n",
    "        return processed_input\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation  \n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_scaled]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_scaled]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76fd2bb",
   "metadata": {},
   "source": [
    "Ani wartości błędu ani wagi końcowe nie zmieniły się - wyniki nadal nie są przydatne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8392c7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba komentarzy: 9.997557902257973e+105\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[False, True, 3, False, 11, 50, 5, 23, 1, 2, 1]])\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "prediction = perceptron.predict(new_data_scaled)\n",
    "print('Przewidywana liczba komentarzy:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2dfe4c",
   "metadata": {},
   "source": [
    "### Zmiana na Float (liczby zmiennoprzecinkowe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "799792f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Błąd (Treningowe): 3.4578085077500045e+212\n",
      "Błąd (Testowe): 7.549259127633164e+210\n",
      "Wagi końcowe:\n",
      "[-4.21821224e+104  3.63378053e+103  3.63378053e+103 -9.49304435e+103\n",
      "  3.63378053e+103 -4.41918151e+105  7.85555541e+105 -7.22310964e+105\n",
      "  2.03371002e+103 -1.27995074e+105 -7.66462458e+105 -2.34631615e+104]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "    \n",
    "    #Dodanie funkcji, która konwertuje każdy input x na liczby zmiennoprzecinkowe (float).\n",
    "    def preprocess_input(self, x):\n",
    "        processed_input = [float(feature) for feature in x]\n",
    "        return processed_input\n",
    "\n",
    "    def predict(self, x): \n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_scaled]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_scaled]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9c27e",
   "metadata": {},
   "source": [
    "Ani wartości błędu ani wagi końcowe nie zmieniły się - wyniki nadal nie są przydatne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54adbd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba komentarzy: 9.997557902257973e+105\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[False, True, 3, False, 11, 50, 5, 23, 1, 2, 1]])\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "prediction = perceptron.predict(new_data_scaled)\n",
    "print('Przewidywana liczba komentarzy:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a3913",
   "metadata": {},
   "source": [
    "### Zmiana na String (wartość tekstowa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db539334",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Błąd (Treningowe): 3.4578085077500045e+212\n",
      "Błąd (Testowe): 7.549259127633164e+210\n",
      "Wagi końcowe:\n",
      "[-4.21821224e+104  3.63378053e+103  3.63378053e+103 -9.49304435e+103\n",
      "  3.63378053e+103 -4.41918151e+105  7.85555541e+105 -7.22310964e+105\n",
      "  2.03371002e+103 -1.27995074e+105 -7.66462458e+105 -2.34631615e+104]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "    \n",
    "    #Dodanie funkcji, która konwertuje każdy input x na wartość tekstową (string).\n",
    "    def preprocess_input(self, x):\n",
    "        processed_input = [string(feature) for feature in x]\n",
    "        return processed_input\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_scaled]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_scaled]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f7b757",
   "metadata": {},
   "source": [
    "Ani wartości błędu ani wagi końcowe nie zmieniły się - wyniki nadal nie są przydatne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fbafba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba komentarzy: 9.997557902257973e+105\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[False, True, 3, False, 11, 50, 5, 23, 1, 2, 1]])\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "prediction = perceptron.predict(new_data_scaled)\n",
    "print('Przewidywana liczba komentarzy:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbdebb6",
   "metadata": {},
   "source": [
    "## Badanie wpływu zmiany ilości inputów na wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e71aeb6",
   "metadata": {},
   "source": [
    "### Inputy związane z obrazkami i grafiką "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a82a591d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0.]\n",
      "Błąd (Treningowe): 1248.9997506177065\n",
      "Błąd (Testowe): 3667.0648727800813\n",
      "Wagi końcowe:\n",
      "[15.32214127 -0.87995224 -0.87995224 -2.07605293 -0.87995224]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12']].values #Wybiera tylko pierwsze cztery zmienne jako input.\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_scaled]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_scaled]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe349fd",
   "metadata": {},
   "source": [
    "Wartość błędu jest znacznie niższa niż w poprzednich przypadkach, co sprawia, że można użyć zmiennych dotyczących grafiki i obrazków w przewidywaniu liczby komentarzy.\n",
    "\n",
    "Wagi końcowe wskazują, że zmienna logiczna dotycząca obecności (lub braku) grafiki ma decydujący (pozytywny) wpływ na liczbę komentarzy. Nieznacznie negatywny wpływ ma natomiast fakt, że obrazki są tylko online. Reszta zmiennych nie ma większego znaczenia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6cb6845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba komentarzy: 17.099158982879082\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[False, True, 3, False]])\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "prediction = perceptron.predict(new_data_scaled)\n",
    "print('Przewidywana liczba komentarzy:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a4f1e0",
   "metadata": {},
   "source": [
    "Przewidywana liczba komentarzy wydaje się być znacznie bardziej prawdopodobna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82768063",
   "metadata": {},
   "source": [
    "### Inputy związane z reakcjami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a029739",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Błąd (Treningowe): 3.9332821978214815e+212\n",
      "Błąd (Testowe): 8.570302834389428e+210\n",
      "Wagi końcowe:\n",
      "[-4.54643811e+104 -4.67308899e+105  8.35130042e+105 -7.72102106e+105\n",
      "  1.90976603e+103 -1.29716793e+105 -8.18843644e+105 -2.27586065e+104]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values #Wybiera tylko zmienne związane z reakcjami jako input.\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_scaled]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_scaled]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae2363",
   "metadata": {},
   "source": [
    "Wartości błędu są bardzo wysokie, co sprawia, że ta architektura nie będzie przydatna w tworzeniu predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "218374fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba komentarzy: 7.677825596125923e+105\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[32, 52, 12, 14, 1, 2, 1]])\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "prediction = perceptron.predict(new_data_scaled)\n",
    "print('Przewidywana liczba komentarzy:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a115b",
   "metadata": {},
   "source": [
    "## Badanie wpływu zmiany sposobu generowania wag początkowych na wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21327582",
   "metadata": {},
   "source": [
    "### Losowe wartości z rozkładu jednorodnego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abf61d48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[ 0.46316904 -0.25632238 -0.88559482  0.25019777 -0.30287936  0.07899875\n",
      "  0.87524405  0.61219516  0.55185888  0.14663932 -0.11837609 -0.10028042]\n",
      "Błąd (Treningowe): 3.391551967058672e+212\n",
      "Błąd (Testowe): 7.404604560019464e+210\n",
      "Wagi końcowe:\n",
      "[-4.17760329e+104  3.59879793e+103  3.59879793e+103 -9.40165431e+103\n",
      "  3.59879793e+103 -4.37663781e+105  7.77992957e+105 -7.15357238e+105\n",
      "  2.01413138e+103 -1.26762859e+105 -7.59083684e+105 -2.32372804e+104]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        #Nowe wagi początkowe.\n",
    "        #Generuje losowe liczby z rozkładu jednorodnego między -1 a 1, zwracając tablicę o rozmiarze = liczba cech + 1.\n",
    "        self.weights = np.random.uniform(-1, 1, size=num_features + 1) \n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_scaled]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_scaled]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c458a",
   "metadata": {},
   "source": [
    "Wartości błędu są bardzo wysokie, co sprawia, że ta architektura nie będzie przydatna w tworzeniu predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f278391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba komentarzy: 9.90131088338964e+105\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[False, True, 3, False, 11, 50, 5, 23, 1, 2, 1]])\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "prediction = perceptron.predict(new_data_scaled)\n",
    "print('Przewidywana liczba komentarzy:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa12f95",
   "metadata": {},
   "source": [
    "### Losowe wartości z rozkładu normalnego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "faecc2b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[-1.40778236 -0.18788927 -1.37830444  0.14021934  1.1306861  -0.86541444\n",
      " -0.8324827   0.7472862  -0.16699829  0.63724627  0.67287172 -0.34900768]\n",
      "Błąd (Treningowe): 3.2264692325961906e+212\n",
      "Błąd (Testowe): 7.044187741921459e+210\n",
      "Wagi końcowe:\n",
      "[-4.07466325e+104  3.51012019e+103  3.51012019e+103 -9.16998879e+103\n",
      "  3.51012019e+103 -4.26879338e+105  7.58822485e+105 -6.97730169e+105\n",
      "  1.96450131e+103 -1.23639304e+105 -7.40379154e+105 -2.26646921e+104]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        #Nowe wagi początkowe.\n",
    "        #Generuje losowe liczby z rozkładu normalnego o średniej równiej 0 i odchyleniu standardowym równym 1, zwracając tablicę o rozmiarze = liczba cech + 1.\n",
    "        self.weights = np.random.normal(0, 1, size=num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_scaled]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_scaled]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47299db1",
   "metadata": {},
   "source": [
    "Wartości błędu są bardzo wysokie, co sprawia, że ta architektura nie będzie przydatna w tworzeniu predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44b9f21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba komentarzy: 9.657333356644634e+105\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[False, True, 3, False, 11, 50, 5, 23, 1, 2, 1]])\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "prediction = perceptron.predict(new_data_scaled)\n",
    "print('Przewidywana liczba komentarzy:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165356b",
   "metadata": {},
   "source": [
    "### Metoda Xavier/Glorot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a44e01dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[-0.00476648 -0.01719371 -0.28053543  0.4559279  -0.29978026  0.42124367\n",
      " -0.24231752 -0.51858146 -0.47336219  0.05976648  0.49625791  0.66880925]\n",
      "Błąd (Treningowe): 3.3562886267358822e+212\n",
      "Błąd (Testowe): 7.327615885485903e+210\n",
      "Wagi końcowe:\n",
      "[-4.15582842e+104  3.58003996e+103  3.58003996e+103 -9.35265020e+103\n",
      "  3.58003996e+103 -4.35382552e+105  7.73937834e+105 -7.11628591e+105\n",
      "  2.00363315e+103 -1.26102134e+105 -7.55127122e+105 -2.31161610e+104]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        #Nowe wagi początkowe.\n",
    "        limit = np.sqrt(6 / (num_features + 1)) #Limit wartości wag na podstawie liczby cech wejściowych.\n",
    "        self.weights = np.random.uniform(-limit, limit, size=num_features + 1) #Generuje losowe wagi z rozkładu jednostajnego w przedziale [-limit, limit].\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_scaled]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_scaled]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77a0c1",
   "metadata": {},
   "source": [
    "Wartości błędu są bardzo wysokie, co sprawia, że ta architektura nie będzie przydatna w tworzeniu predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4a4f5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba komentarzy: 9.84970241330213e+105\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[False, True, 3, False, 11, 50, 5, 23, 1, 2, 1]])\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "prediction = perceptron.predict(new_data_scaled)\n",
    "print('Przewidywana liczba komentarzy:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b2c67",
   "metadata": {},
   "source": [
    "## Badanie wpływu zmiany sposobu przetwarzania danych wejściowych na wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e79080",
   "metadata": {},
   "source": [
    "### Normalizacja danych wejściowych\n",
    "Zastosowanie funkcji MinMaxScaler jako skalera zamiast funkcji StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b981a1bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Błąd (Treningowe): 649.2942800148435\n",
      "Błąd (Testowe): 3123.2055766658464\n",
      "Wagi końcowe:\n",
      "[ 22.64471859  -4.89679371  -4.89679371 -20.470875    -4.89679371\n",
      " 179.63017432  20.21922841 159.02609152  14.71578482  36.36817021\n",
      " 157.32006213  -0.47633756]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Wykonuje normalizację danych przy użyciu skalera typu MinMaxScaler z biblioteki scikit-learn.\n",
    "#Przekształca wartości danych w taki sposób, aby mieściły się w zakresie (0, 1).\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_normalized.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_normalized, y_train)\n",
    "\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_normalized]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_normalized]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d7028e",
   "metadata": {},
   "source": [
    "Wartość błędu jest najniższa ze wszystkich dotychczasowych przypadków.\n",
    "\n",
    "Wagi końcowe pokazują następujące zależności:\n",
    "- Największy pozytywny wpływ na liczbę komentarzy mają zmienne dotyczące reakcji \"LOVE\", \"HAHA\" i \"WRR\".\n",
    "- Mniejszy pozytywny wpływ mają zmienne dotyczące obecności grafiki oraz reakcji \"CRY\", \"WOW\" i \"HUG\".\n",
    "- Największy negatywny wpływ na liczbę komentarzy ma zmienna, która wskazuję na to czy obrazki są dostępne tylko online.\n",
    "- Wpływ reszty zmiennych jest stosunkowo marginalny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25dfcb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba komentarzy: 53.78529467215922\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[False, True, 3, False, 11, 50, 5, 23, 1, 2, 1]])\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "prediction = perceptron.predict(new_data_scaled)\n",
    "print('Przewidywana liczba komentarzy:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e84b4",
   "metadata": {},
   "source": [
    "Na tle dotychczasowych iteracji, wynik proponowany przez tę architekturę wydaję się być najbardziej realistyczny. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f61fd6",
   "metadata": {},
   "source": [
    "### Przekształcenie logarytmiczne danych wejściowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d45ed9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Błąd (Treningowe): 622.5841139734817\n",
      "Błąd (Testowe): 3168.280043841729\n",
      "Wagi końcowe:\n",
      "[14.23814908 -8.9054584  -8.9054584  -6.4065441  -8.9054584   8.65545831\n",
      " -6.06927855 16.75857496  0.09396205  1.81596855 70.1839925   0.4945371 ]\n"
     ]
    }
   ],
   "source": [
    "df1['Z9'] = df1['Z9'].astype('int64')\n",
    "df1['Z10'] = df1['Z9'].astype('int64')\n",
    "df1['Z12'] = df1['Z9'].astype('int64')\n",
    "\n",
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Logarytmowanie danych treningowych i testowych za pomocą funkcji log1p z biblioteki NumPy.\n",
    "X_train_log = np.log1p(X_train)\n",
    "X_test_log = np.log1p(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_log.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_log, y_train)\n",
    "\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_log]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_log]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c45dac",
   "metadata": {},
   "source": [
    "Przekształcenie logarytmiczne również wydaje się znacznie poprawiać otrzymywane wyniki. W porównaniu do normalizacji ten sposób przetwarzania danych wejściowych nieznacznie zmniejsza błąd w danych treningowych, jednocześnie nieznacznie zwiększając błąd w danych testowych.\n",
    "\n",
    "Wagi wskazują, że na liczbę komentarzy pozytywne wpływa przede wszystkim ilość reakcji \"WRR\", a w dalszej kolejność ilość reakcji \"HAHA\", \"LOVE\" oraz obecność grafiki. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a4595eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba komentarzy: 105.86112959926703\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[False, True, 3, False, 11, 50, 5, 23, 1, 2, 1]])\n",
    "new_data_log = np.log1p(new_data)\n",
    "prediction = perceptron.predict(new_data_log)\n",
    "print('Przewidywana liczba komentarzy:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080e35a",
   "metadata": {},
   "source": [
    "Przewidywana liczba komentarzy jest dwa razy większa niż w poprzedniej architekturze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c04f1",
   "metadata": {},
   "source": [
    "## Badanie wpływu zmiany sposobu przetwarzania danych wyjściowych na wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb0a3b",
   "metadata": {},
   "source": [
    "### Standaryzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a119bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Błąd (Treningowe): 2.3899576010134094e+209\n",
      "Błąd (Testowe): 5.217874036016803e+207\n",
      "Wagi końcowe:\n",
      "[-1.10897832e+103  9.55329793e+101  9.55329793e+101 -2.49574459e+102\n",
      "  9.55329793e+101 -1.16181363e+104  2.06524474e+104 -1.89897320e+104\n",
      "  5.34667340e+101 -3.36502182e+103 -2.01504856e+104 -6.16852257e+102]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "#Standaryzacja danych wyjściowych\n",
    "scaler = StandardScaler()\n",
    "y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train_scaled)\n",
    "\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_scaled]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_scaled]\n",
    "\n",
    "mse_train = np.mean((y_pred_train- y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test- y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd2c151",
   "metadata": {},
   "source": [
    "Wartości błędu są bardzo wysokie, co sprawia, że ta architektura nie będzie przydatna w tworzeniu predykcji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0f8f9",
   "metadata": {},
   "source": [
    "### Normalizacja "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7ca55dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Błąd (Treningowe): 2.3899576010134094e+209\n",
      "Błąd (Testowe): 5.217874036016803e+207\n",
      "Wagi końcowe:\n",
      "[-7.86979896e+101  6.77944128e+100  6.77944128e+100 -1.77109036e+101\n",
      "  6.77944128e+100 -8.24474162e+102  1.46558870e+103 -1.34759508e+103\n",
      "  3.79423510e+100 -2.38796780e+102 -1.42996727e+103 -4.37745550e+101]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "#Normalizacja danych wyjściowych\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train_scaled)\n",
    "\n",
    "y_pred_train_scaled = np.array([perceptron.predict(x) for x in X_train_scaled])\n",
    "y_pred_test_scaled = np.array([perceptron.predict(x) for x in X_test_scaled])\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b73b96",
   "metadata": {},
   "source": [
    "Wartości błędu są bardzo wysokie, co sprawia, że ta architektura nie będzie przydatna w tworzeniu predykcji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430c00a",
   "metadata": {},
   "source": [
    "### Przekształcenie logarytmiczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3c3c573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Błąd (Treningowe): 2.3899576010134094e+209\n",
      "Błąd (Testowe): 5.217874036016803e+207\n",
      "Wagi końcowe:\n",
      "[-1.31078578e+102  1.12917690e+101  1.12917690e+101 -2.94991025e+101\n",
      "  1.12917690e+101 -1.37323585e+103  2.44106975e+103 -2.24454078e+103\n",
      "  6.31963973e+100 -3.97737508e+102 -2.38173907e+103 -7.29104573e+101]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "#Logarytmowanie danych wyjściowych\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train_log)\n",
    "\n",
    "y_pred_train_log = [perceptron.predict(x) for x in X_train_scaled]\n",
    "y_pred_test_log = [perceptron.predict(x) for x in X_test_scaled]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedf795",
   "metadata": {},
   "source": [
    "Wartości błędu są bardzo wysokie, co sprawia, że ta architektura nie będzie przydatna w tworzeniu predykcji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8d7e4",
   "metadata": {},
   "source": [
    "## Badanie wpływu zmiany sposobu przetwarzania danych wejściowych i wyjściowych na wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32712c72",
   "metadata": {},
   "source": [
    "### Normalizacja danych wejściowych i wyjściowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16152457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Błąd (Treningowe): 2.3899576010134094e+209\n",
      "Błąd (Testowe): 5.217874036016803e+207\n",
      "Wagi końcowe:\n",
      "[-7.86979896e+101  6.77944128e+100  6.77944128e+100 -1.77109036e+101\n",
      "  6.77944128e+100 -8.24474162e+102  1.46558870e+103 -1.34759508e+103\n",
      "  3.79423510e+100 -2.38796780e+102 -1.42996727e+103 -4.37745550e+101]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Normalizacja danych wejściowych\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "#Normalizacja danych wyjściowych\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train_scaled)\n",
    "\n",
    "y_pred_train_scaled = np.array([perceptron.predict(x) for x in X_train_scaled])\n",
    "y_pred_test_scaled = np.array([perceptron.predict(x) for x in X_test_scaled])\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a5c7f",
   "metadata": {},
   "source": [
    "Wartości błędu są bardzo wysokie, co sprawia, że ta architektura nie będzie przydatna w tworzeniu predykcji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c57df2",
   "metadata": {},
   "source": [
    "### Przekształcenie logarytmiczne wejściowych i wyjściowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52950538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Błąd (Treningowe): 2.3899576010134094e+209\n",
      "Błąd (Testowe): 5.217874036016803e+207\n",
      "Wagi końcowe:\n",
      "[-1.31078578e+102  1.12917690e+101  1.12917690e+101 -2.94991025e+101\n",
      "  1.12917690e+101 -1.37323585e+103  2.44106975e+103 -2.24454078e+103\n",
      "  6.31963973e+100 -3.97737508e+102 -2.38173907e+103 -7.29104573e+101]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Logarytmowanie danych wejściowych\n",
    "X_train_log = np.log1p(X_train)\n",
    "X_test_log = np.log1p(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "#Logarytmowanie danych wyjściowych\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_scaled.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_scaled, y_train_log)\n",
    "\n",
    "y_pred_train_log = [perceptron.predict(x) for x in X_train_scaled]\n",
    "y_pred_test_log = [perceptron.predict(x) for x in X_test_scaled]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa371d",
   "metadata": {},
   "source": [
    "Wartości błędu są bardzo wysokie, co sprawia, że ta architektura nie będzie przydatna w tworzeniu predykcji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9abec25",
   "metadata": {},
   "source": [
    "# Finalna architektura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e4dbe96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi początkowe:\n",
      "[-0.75877391 -0.01043738  1.72326738 -0.17073291 -0.40685738 -0.63302118\n",
      " -0.07536434  0.59451535  0.22301761  0.1512672  -1.30690894  0.54501815]\n",
      "Błąd (Treningowe): 649.2985420388036\n",
      "Błąd (Testowe): 3123.27578099641\n",
      "Wagi końcowe:\n",
      "[ 22.64486099  -5.34182676  -3.608122   -20.47909052  -5.73824676\n",
      " 179.46125369  20.25523492 160.01880244  14.75047674  36.46792395\n",
      " 156.41042624  -0.44806161]\n"
     ]
    }
   ],
   "source": [
    "X = df1[['Z9', 'Z10', 'Z11', 'Z12', 'Z14', 'Z15', 'Z16', 'Z17', 'Z18', 'Z19', 'Z20']].values\n",
    "y = df1['Z21'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Normalizacja danych wejściowych przy użyciu skalera typu MinMaxScaler.\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        #Losowe wartości z rozkładu normalnego jako wagi początkowe.\n",
    "        self.weights = np.random.normal(0, 1, size=num_features + 1)\n",
    "        print(\"Wagi początkowe:\")\n",
    "        print(self.weights)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def activate(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        activation = np.dot(self.weights, x)\n",
    "        return activation\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                y_pred = self.predict(x)\n",
    "                error = y_true - y_pred\n",
    "                self.weights += self.learning_rate * error * np.insert(x, 0, 1)\n",
    "\n",
    "perceptron = Perceptron(num_features=X_train_normalized.shape[1], learning_rate=0.01, num_epochs=100)\n",
    "perceptron.train(X_train_normalized, y_train)\n",
    "\n",
    "y_pred_train = [perceptron.predict(x) for x in X_train_normalized]\n",
    "y_pred_test = [perceptron.predict(x) for x in X_test_normalized]\n",
    "\n",
    "mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print('Błąd (Treningowe):', mse_train)\n",
    "print('Błąd (Testowe):', mse_test)\n",
    "\n",
    "trained_weights = perceptron.get_weights()\n",
    "print(\"Wagi końcowe:\")\n",
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a766d",
   "metadata": {},
   "source": [
    "Tak opracowana architektura ma najmniejszy błąd średniokwadratowy. <br>\n",
    "- Wykorzystuje normalizacje danych wejściowych za pomocą MinMaxScaler\n",
    "- Wagi początkowe to losowe wartości z rozkładu normalnego "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbe96b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba komentarzy: 55.182755104148505\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[False, True, 3, False, 11, 50, 5, 23, 1, 2, 1]])\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "prediction = perceptron.predict(new_data_scaled)\n",
    "print('Przewidywana liczba komentarzy:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e8e94",
   "metadata": {},
   "source": [
    "## Zestaw finalnie opracowanych wag\n",
    "\n",
    "Wagi końcowe:\n",
    "[ 22.64486099  -5.34182676  -3.608122   -20.47909052  -5.73824676 <br>\n",
    " 179.46125369  20.25523492 160.01880244  14.75047674  36.46792395 <br>\n",
    " 156.41042624  -0.44806161]\n",
    " \n",
    " Wagi końcowe pokazują następujące zależności:\n",
    "- Największy pozytywny wpływ na liczbę komentarzy mają zmienne dotyczące reakcji \"LOVE\", \"HAHA\" i \"WRR\".\n",
    "- Mniejszy pozytywny wpływ mają zmienne dotyczące obecności grafiki oraz reakcji \"CRY\", \"WOW\" i \"HUG\".\n",
    "- Największy negatywny wpływ na liczbę komentarzy ma zmienna, która wskazuję na to czy obrazki są dostępne tylko online.\n",
    "- Wpływ reszty zmiennych jest stosunkowo marginalny."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
